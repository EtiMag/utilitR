# Importer des fichiers au format Parquet {#importparquet}

## Tâches concernées et recommandations

- L'utilisateur souhaite importer et exploiter dans `R` des données stockées au format **Parquet**.
- L'utilisateur souhaite convertir des données au format **Parquet**.

::: {.callout-recommandation .icon}

- Il est recommandé d'utiliser le format **Parquet** pour stocker des données volumineuses, car il est plus compact que le format csv. Le **package** [`arrow`](https://arrow.apache.org/docs/r/) permet de lire, d'écrire et de manipuler simplement les fichiers au format **Parquet** avec `R`.
- Il est essentiel de travailler avec la dernière version d'`arrow` et de `R` car le _package_ `arrow` est en cours de développement.
- Il est recommandé de partitionner les fichiers **Parquet** lorsque les données sont volumineuses et lorsque les données peuvent être partitionnées selon une variable logique (département, secteur, année...).
- Lorsqu'on importe des données volumineuses, il est recommandé de sélectionner les observations (avec `filter`) et les variables (avec `select`) pour limiter la consommation de mémoire vive.

:::

Note: cette fiche n'a pas vocation à être exhaustive sur le format Parquet, mais plutôt à lister les points saillants à retenir lorsqu'un statisticien souhaite travailler avec ce format de fichier.  

## Qu'est-ce que Parquet et pourquoi s'en servir?

### Qu'est-ce que le format Parquet?

**Parquet** est un format de stockage de données, au même titre que les fichiers CSV, RDS, FST... Ce format n'est pas nouveau (création en 2013), mais il a gagné en popularité dans le monde de la _data science_ au cours des dernières années, notamment grâce au projet _open-source_ [Apache arrow](https://arrow.apache.org/).

Le format Parquet présente plusieurs avantages cruciaux qui en font un concurrent direct du format csv:

- il compresse efficacement les données, ce qui le rend très adapté au stockage de données volumineuses;
- il est conçu pour être indépendant d'un logiciel: on peut lire des fichiers Parquet avec `R`, Python, C++, Java...
- il est conçu pour que les données puissent être chargées très rapidement en mémoire.


### Caractéristiques du format Parquet 

Le format Parquet présente trois caractéristiques importantes du point de l'utilisateur:

- __Parquet encode les données en un format binaire__. Cela signifie qu'un fichier Parquet n'est pas lisible par un humain: contrairement au format `csv`, on ne peut pas ouvrir un fichier Parquet avec Excel, LibreOffice ou Notepad pour jeter un coup d'oeil au contenu.

- Parquet repose sur un **stockage orienté colonne**. Ainsi seront stockées dans un premier temps toutes les données de la première colonne de la table, puis seulement dans un second temps les données de la deuxième colonne et ainsi de suite... [Le blog d'upsolver](https://www.upsolver.com/blog/apache-parquet-why-use) fournit une illustration pour bien visualiser la différence :  

```{r, echo = FALSE, fig.cap = "Différence entre le stockage orienté ligne et colonne"}
knitr::include_graphics("../pics/parquet/stockage_colonne.png")
```

- **Un fichier Parquet contient à la fois les données et des métadonnées**. Ces métadonnées écrites à la fin du fichier enregistrent une description du fichier (appelé **schéma**). Ces métadonnées contiennent notamment le type de chaque colonne (entier/réel/caractère). Ce sont ces métadonnées qui font en sorte que la lecture des données Parquet soit optimisée et sans risque d’altération (voir [ici](https://parquet.apache.org/docs/file-format/metadata/) pour en savoir plus).

Dans un contexte analytique, cette organisation des données génère plusieurs avantages dont les principaux sont: 

- **Un gain de vitesse lors de la lecture des données pour un usage statistique**: `R` peut extraire directement les colonnes demandées sans avoir à scanner toutes les lignes comme ce serait le cas avec un fichier `csv` ;
- **La possibilité d'avoir un haut niveau de compression**. Le taux de compression moyen par rapport au format `csv` est souvent compris entre 5 et 10. Pour des fichiers volumineux il est même possible d'avoir des taux de compression bien supérieurs.

Inversement, le format Parquet présente deux contraintes inhabituelles pour les utilisateurs des autres formats (CSV, SAS, FST...):

- Il n'est pas possible de charger seulement quelques lignes d'un fichier Parquet: on importe nécessairement des colonnes entières;
- Il n'est pas possible d'ouvrir un fichier Parquet avec Excel, LibreOffice ou Notepad.

Pour en savoir plus notamment sur la comparaison entre les formats Parquet et csv, consultez 
[le chapitre sur le sujet](https://pythonds.linogaliana.fr/reads3/#le-format-parquet) dans le cours de l'ENSAE _"Python pour la data science"_. 
Grâce aux travaux du projet Arrow, **les fichiers aux format Parquet sont inter-opérables** c'est-à-dire qu'ils peuvent être lus par plusieurs langages informatiques : [C](https://arrow.apache.org/docs/c_glib/), [C++](https://arrow.apache.org/docs/cpp/), [C#](https://github.com/apache/arrow/blob/main/csharp/README.md), [Go](https://godoc.org/github.com/apache/arrow/go/arrow), [Java](https://arrow.apache.org/docs/java/), [JavaScript](https://arrow.apache.org/docs/js/), [Julia](https://arrow.juliadata.org/stable/), [MATLAB](https://github.com/apache/arrow/blob/main/matlab/README.md), [Python](https://arrow.apache.org/docs/python/), [Ruby](https://github.com/apache/arrow/blob/main/ruby/README.md), [Rust](https://docs.rs/crate/arrow/) et bien entendu [R](https://arrow.apache.org/docs/r/). Le format Parquet est donc particulièrement adapté aux chaînes de traitement qui font appel à plusieurs langages (exemples: manipulation de données avec `R` puis _machine learning_ avec Python).

S'il est très efficace pour l'analyse de données, **Parquet est en revanche peu adapté à l'ajout de données en continu ou à la modification fréquente de données existantes**.  
Pour cette utilisation, le statisticien privilégiera un système de gestion de base de données comme par exemple [`PostgreSQL`](https://www.postgresql.org/).

## Écrire des fichiers Parquet

### Cas simple: écrire un seul fichier Parquet

Les tables Parquet sont encore loin d'être majoritaires dans les liens de téléchargement notamment face au format csv. C'est la raison pour laquelle, nous allons dans cette section dérouler **le processus pour obtenir un fichier Parquet à partir d'un fichier csv.**  

Les tables Parquet sont encore loin d'être majoritaires dans les liens de téléchargement notamment face au format csv. C'est la raison pour laquelle, nous allons dans cette section dérouler **le processus pour obtenir un fichier Parquet à partir d'un fichier csv.** Cet exemple repose sur un fichier volumineux disponible sur le site de l'Insee.  

Dans un premier temps, on importe le fichier plat avec la fonction **fread()** du _package_ **data.table**, conformément aux recommandations de [la fiche sur les imports de fichiers plats](https://www.book.utilitr.org/03_fiches_thematiques/fiche_import_fichiers_plats). On obtient un objet `data.table` en mémoire. Dans un second temps, on exporte ces données en format Parquet avec la fonction `write_parquet()` du _package_ `arrow`. Comme vous pouvez le voir ci-dessous, ces deux étapes sont réalisées en un seul temps, ce qui réduit l'utilisation de ressources informatiques. 

```{r, eval=FALSE}
library(data.table)
library(magrittr)
library(arrow)

# Décompression du fichier zip
unzip("Data/dpt2021_csv.zip", exdir = "Data")
# Création du dossier "Data"
dir.create("Data")
# Conversion du fichier csv au format parquet
write_parquet(
  x = fread("Data/dpt2021.csv"), # Utilisation de la fonction fread()
  sink = "Data/dpt2021.parquet"
)
```

À l'issue de cette conversion, on peut noter que **le fichier Parquet créé occupe un espace de stockage 10 fois moins important que le fichier csv initial (7,4 Mo contre 76,3 Mo) !**

Pour les exemples qui suivent dans cette fiche, on utilise un fichier de [la Base Permanente des Équipements de l'Insee](https://www.insee.fr/fr/statistiques/3568629) que l'on va convertir au format **Parquet**.  
Vous pouvez télécharger ce fichier avec le package [`doremifasol`](https://inseefrlab.github.io/DoReMIFaSol/index.html) et plus particulièrement la fonction [`telechargerDonnees()`](https://inseefrlab.github.io/DoReMIFaSol/reference/telechargerDonnees.html) :

```{r, eval=FALSE}
# remotes::install_github("InseeFrLab/doremifasol", build_vignettes = TRUE)
library(doremifasol)
library(arrow)  

# Création du dossier "Data"
dir.create("Data")

# Téléchargement des données de la BPE
donnees_BPE <- telechargerDonnees("BPE_ENS", date = 2021)

write_parquet(
  x = donnees_BPE,
  sink = "Data/BPE_ENS.parquet"
)
```

### Cas complexe: écrire un fichier Parquet partitionné


Le package `arrow` présente une fonctionnalité supplémentaire qui consiste à créer et lire un fichier **Parquet partitionné**. Le partitionnement des fichiers Parquet présente des avantages pratiques qui sont expliqués dans la suite de cette fiche (voir partie [Lire un fichier Parquet avec `R`]).

Partitionner un fichier revient à le "découper" selon une clé de partitionnement, qui prend la forme d'une ou de plusieurs variables. Cela signifie en pratique que l'ensemble des données sera stockée sous forme d'un grand nombre de fichiers Parquet (un fichier par valeur des variable de partitionnement). Par exemple, il est possible de partitionner un fichier national par département: on obtient alors un fichier Parquet par département.

::: {.callout-conseil .icon}
- **Il est important de bien choisir les variables de partitionnement** d'un fichier **Parquet**. Il faut choisir des variables faciles à comprendre et qui soient cohérentes avec l'usage des données (année, département, secteur...). En effet, un partitionnement bien construit induit par la suite des gains d'efficacité sur les traitements et facilite la maintenance du fichier sur le long terme.
- **Il est inutile de partitionner des données de petite taille**. Si les données dépassent quelques millions d'observations et/ou si leur taille en CSV dépasse quelques giga-octets, il est utile de partitionner.
- **Il ne faut pas partitionner les données en trop de fichiers**. En pratique, il est rare d'avoir besoin de plus qu'une ou deux variables de partitionnement.
:::

Pour créer des fichiers **Parquet** partitionnés, il faut utiliser la fonction [`write_dataset()`](https://arrow.apache.org/docs/r/reference/write_dataset.html) du _package_ `arrow`. Voici ce que ça donne sur le fichier de la BPE :

```{r, eval = FALSE}
write_dataset(
  dataset = donnees_BPE, 
  path = "Data/", 
  partitioning = c("REG"), # la variable de partitionnement
  format="parquet"
)
```

Avec cette instruction, on a créé autant de répertoires que de modalités différentes de la variable `REG`. Vous pouvez noter la structure des dossiers nommés `REG==[valeur]`.

```{r, echo = FALSE, fig.cap = "Arborescence d'un fichier Parquet partitionné"}
knitr::include_graphics("../pics/parquet/fichier_partition.png")
```

## Lire un fichier Parquet avec `R`

La fonction [`read_parquet()`](https://arrow.apache.org/docs/r/reference/read_parquet.html) permet d'importer des fichiers Parquet dans `R`. Elle possède un argument très utile `col_select` qui permet de sélectionner les variables à importer (par défaut toutes). Cet argument accepte soit une liste de noms de variables soit [une expression dite de `tidy selection` issue du *tidyverse*](https://dplyr.tidyverse.org/reference/dplyr_tidy_select.html).

Pour utiliser `read_parquet()`, il faut charger le *package* `arrow` :

```{r, eval=FALSE}
library(arrow)
```

La fonction [`read_parquet()`](https://arrow.apache.org/docs/r/reference/read_parquet.html) du _package_ `arrow` permet d'importer des fichiers Parquet dans `R`. Elle possède un argument très utile `col_select` qui permet de sélectionner les variables à importer (par défaut toutes). Cet argument accepte soit une liste de noms de variables, soit [une expression dite de `tidy selection` issue du *tidyverse*](https://dplyr.tidyverse.org/reference/dplyr_tidy_select.html).

```{r, eval = FALSE}
donnees <- arrow::read_parquet("Data/BPE_ENS.parquet")
```

- Exemple en ne sélectionnant que quelques variables à l'aide d'un vecteur de caractères :

```{r, eval = FALSE}
donnees <- arrow::read_parquet(
  "Data/BPE_ENS.parquet",
  col_select = c('AN','REG','DEP','SDOM','TYPEQU','NB_EQUIP')
) 
```

- Exemple en ne sélectionnant que quelques variables à l'aide d'une `tidy selection` :

```{r, eval = FALSE}
donnees <- arrow::read_parquet(
  "Data/BPE_ENS.parquet",
  col_select = starts_with("DEP")
) 
```

Dans les trois cas, le résultat obtenu est un objet directement utilisable dans R. :tada:  

La méthode présentée dans cette section est valable pour les fichiers peu volumineux. Elle implique en effet d'importer l'intégralité d'un fichier Parquet dans la mémoire vive de votre espace de travail avant de pouvoir travailler dessus. Il est possible d'effectuer des requêtes plus efficacement sur des fichiers Parquet, c'est ce que nous allons voir dans les sections suivantes.

## Exploiter un fichier Parquet avec le package dplyr

Si le statisticien souhaite travailler sur des fichiers plus volumineux (par exemple [celui des données du recensement de la population 1968-2019](https://www.insee.fr/fr/statistiques/6671801) de 3,2 Go et qui contient plus de 51,5 millions de lignes et 18 colonnes), il peut se heurter à **un manque de mémoire vive** s'il souhaite importer dans `R` l'intégralité de la table avant de pouvoir l'exploiter.  

## Exploiter un fichier Parquet avec le _package_ `dplyr`
- Exemple avec une table peu volumineuse :  

```{r, eval=FALSE}
library(dplyr)
library(arrow)

open_dataset("Data/BPE_ENS.parquet") |>
  filter(REG == "76") |>
  group_by(DEP) |>
  collect()
```

Avec cette syntaxe, la requête va utiliser seulement les variables du fichier **Parquet** dont elle a besoin (en l'occurence `REG`, `DEP` et `NB_EQUIP`).

- Exemple avec une table volumineuse (Recensements 1968-2019, suivre ce [lien](https://gist.github.com/ddotta/acf6add0f2328f077791461ef4f37b84) pour obtenir le code qui permet de générer "Ficdep19.parquet" de façon reproductible) :  

```{r, eval=FALSE}
library(dplyr)

open_dataset("Data/Ficdep19.parquet") |>
  filter(DEP_RES_21 == "11") |>
  group_by(SEXE) |>
  summarise(total = sum(pond)) |>
  as.data.frame() |>
  collect()
```

Cette instruction s'exécute sur mon espace de travail en un peu plus de 2 secondes.

## Exploiter un fichier Parquet avec le _package_ `duckdb`

Dans le cas de fichiers volumineux, il est également possible de les requêter avec le langage `SQL` grâce au package [`duckdb`](https://duckdb.org/docs/api/r.html). Cette méthode est basée sur le moteur portable `DuckDB` qui permet à n'importe quel ordinateur d'accéder à des performances d'un moteur de base de données classique qui utilise un serveur. Pour plus d'informations sur la façon d'exécuter des requêtes sur des bases de données, consultez [cette fiche](https://www.book.utilitr.org/03_fiches_thematiques/fiche_connexion_bdd#ex%C3%A9cuter-des-requ%C3%AAtes). Il faut noter que la méthode présentée ici est encore un peu plus efficace que celle présentée avec dans la section précédente avec les fonctions de `dplyr`. 

En `R`, il faut charger le package `duckdb` :

```{r, eval = FALSE}
library(duckdb)
```

- Exemple avec une table peu volumineuse : 

```{r, eval = FALSE}
con <- dbConnect(duckdb::duckdb())
 
dbGetQuery(con, "SELECT SUM(NB_EQUIP) FROM 'Data/BPE_ENS.parquet'
                 WHERE REG='76'
                 GROUP BY DEP")
```
                
- Exemple avec une table volumineuse (RP 1968-2019) : 

```{r, eval = FALSE}
con <- dbConnect(duckdb::duckdb())
 
dbGetQuery(con, "SELECT SUM(POND) FROM 'Data/Ficdep19.parquet'
                 WHERE DEP_RES_21='11'
                 GROUP BY SEXE")
```
Cette instruction s'exécute sur mon espace de travail en environ 0.5 secondes !

## Exploiter un fichier Parquet partitionné

### Quel est l'intérêt d'utiliser des fichiers Parquet partitionnés?

Les _packages_ `arrow` et `duckdb` présentent une grande différence avec les _packages_ standard de manipulation de données comme `dplyr` ou `data.table`: lorsqu'on exécute une requête sur une table de données, ces _packages_ ne se contentent pas d'exécuter les commandes une à une, dans l'ordre du code, mais analysent la requête pour **optimiser le plan d'exécution de la requête**. En pratique, cela signifie qu'`arrow` et `duckdb` essaient de n'importer que les observations nécessaires à la requête, de ne conserver que les colonnes nécessaires au calcul, etc. Cette optimisation du plan d'exécution (appelée _predicate push-down_) permet d'accélérer les traitements et de réduire la consommation de ressources informatiques.

**Utiliser un fichier Parquet partitionné facilite ce travail d'optimisation.** Comme mentionné plus haut, il n'est pas possible de charger seulement quelques lignes d'un fichier Parquet: on importe nécessairement des colonnes entières. Toutefois, lorsque le fichier Parquet est partitionné, `arrow` est capable de filtrer les lignes à importer à l'aide des clés de partitionnement, ce qui permet d'accélérer l'importation des données.

Exemple: imaginons que la Base Permanente des Équipements soit stockée sous la forme d'un fichier Parquet partitionné par région (`REG`), et qu'on veuille compter le nombre d'équipements de chaque type dans chaque département de la région Hauts-de-France (`REG == "32"`). On utilisera le code suivant:

```{r, eval = FALSE}
open_dataset(
  "Data/",
  partitioning = arrow::schema(REG = arrow::utf8())
) %>% 
  filter(REG == "32") %>%
  select(DEP, TYPEQU, NB_EQUIP) %>%
  group_by(DEP, TYPEQU) %>%
  summarise(nb_equipements = sum(NB_EQUIP)) %>%
  collect()
```


Au moment d'exécuter cette requête, `arrow` va utiliser la variable de partitionnement pour ne lire que la partie `REG == "32"` du fichier partitionné (donc seulement une partie des observations). Autrement dit, le fait que le fichier Parquet soit partitionné accélère la lecture des données.

En conclusion, l'utilisation des fichiers Parquet partitionné présente trois avantages :  
- Elle permet de travailler sur des fichiers **Parquet** de plus petite taille et de consommer moins de mémoire vive;
- Elle facilite la maintenance des fichiers : seuls les fichiers concernés seront affectés si une mise à jour des données devaient avoir lieu (par exemple sur la région "76");
- Elle fait gagner du temps dans l'exécution des requêtes sur les fichiers volumineux (par rapport à un fichier **Parquet** unique).* [Page officielle du projet Arrow](https://arrow.apache.org/)

### Comment bien utiliser les fichiers Parquet partitionnés?

La fonction [`open_dataset()`](https://arrow.apache.org/docs/r/reference/open_dataset.html) permet d’ouvrir une connexion vers un fichier Parquet partitionné. Une fois que la connexion est établie avec le fichier partitionné, il est possible de l'utiliser exactement comme une table de données chargée en mémoire. Voici un exemple de code:


```{r, eval = FALSE}
open_dataset(
  "Data/",
  hive_style = TRUE,
  partitioning = arrow::schema(REG = arrow::utf8())
) |>
  filter(REG == "32") |> # Ici, on filtre selon la clé de partitionnement
  select(DEP, TYPEQU, NB_EQUIP) %>%
  group_by(DEP) |>
  summarise(nb_total = sum(NB_EQUIP)) |>
  collect()
```

Pour bien utiliser un fichier Parquet partitionné, il est recommandé de suivre les trois conseils suivants:

- Afin de tirer au mieux profit du partitionnement, il est conseillé de **filtrer les données** de préférence **selon les variables de partitionnement** définies (dans notre exemple, la région);
- Il est fortement recommandé de spécifier le type des variables de partitionnement avec l'argument `partitioning`. Cela évite des erreurs typiques: le code du département est interprété à tort comme un nombre et aboutit à une erreur à cause de la Corse... Cet argument s'utilise en construisant un schéma qui précise le type de chacune des variables de partitionnement:

    ```{r, eval = FALSE}
    open_dataset(
      "Data/",
      partitioning = arrow::schema(variable1 = arrow::utf8(), variable2 = arrow::int16())
    )
    ```

    Les types les plus fréquents sont: nombre entier (`int8()`, `int16()`, `int32()`, `int64()`), nombre réel (`float()`, `float32()`, `float64()`), et chaîne de caractère (`utf8()`, `large_utf8()`). Il existe beaucoup d'autres types, ous pouvez en consulter la liste en exécutant `?arrow::float`.
- Il est recommandé de définir les deux options suivantes au début de votre script. Cela autorise `arrow` à utiliser plusieurs processeurs à la fois, ce qui accélère les traitements:

    ```{r, eval = FALSE}
    # Autoriser arrow à utiliser plusieurs processeurs en même temps
  options(arrow.use_threads = TRUE)
    # Définir le nombre de processeurs utilisés par arrow
    # 10 processeurs sont suffisants dans la plupart des cas
    arrow:::set_cpu_count(10)
    ```

## Pour en savoir plus

* [Page officielle de duckdb](https://duckdb.org/)
* [Apache Parquet pour le stockage de données volumineuses](https://www.cetic.be/Apache-Parquet-pour-le-stockage-de-donnees-volumineuses)